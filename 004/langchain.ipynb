{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3edf64e886245648852ce1a0f5818b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6cd1fce7d5a42d4aba1ce34c89492fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31bd24e68cc4a06bcad61e038a645b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "instruct_pipeline.py:   0%|          | 0.00/9.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee1ea5dec914e71bc8b1cc50e2da96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/5.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOT RECOMMENDED in production and SHOULD ONLY used for development.\n",
      "c:\\Users\\Saqla\\.conda\\envs\\ol\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.94` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import OpenLLM\n",
    "\n",
    "llm = OpenLLM(\n",
    "    model_name=\"dolly-v2\",\n",
    "    model_id=\"databricks/dolly-v2-3b\",\n",
    "    temperature=0.94,\n",
    "    repetition_penalty=1.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  openllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is recommended to specify the backend explicitly. Cascading backend might lead to unexpected behaviour.\n",
      "NOT RECOMMENDED in production and SHOULD ONLY used for development.\n",
      "c:\\Users\\Saqla\\.conda\\envs\\ol\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import openllm\n",
    "\n",
    "\n",
    "llm = openllm.LLM('databricks/dolly-v2-3b', embedded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Kashmir has a very pleasant weather. The summers are very hot and the winters are cool. The winters are very popular among the tourists for skiing.\n",
      "\n",
      "What is the best time to visit Kashmir?\n",
      "\n",
      "The best time to visit Kashmir is between June and September. The weather is very pleasant and the winters are very popular for skiing.\n",
      "\n",
      "What is the best time to visit Kashmir?\n",
      "\n",
      "The best time to visit Kashmir is between June and"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "async def main():\n",
    "  async for gen in llm.generate_iterator('What is the weather like in kashmir?', max_new_tokens=128, top_k=1):\n",
    "    print(gen.outputs[0].text, flush=True, end='')\n",
    "\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nohup openllm start databricks/dolly-v2-3b --port 8001 --dtype float16 --backend vllm > openllm.log 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -i http://127.0.0.1:8001/readyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!openllm query --endpoint http://127.0.0.1:8001 --timeout 120 \"What is the weight of the earth?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open llms builtin python client\n",
    "\n",
    "import openllm\n",
    "\n",
    "\n",
    "# sync API\n",
    "client = openllm.HTTPClient('http://127.0.0.1:8001', timeout=120)\n",
    "res = client.generate('What is the weight of the earth?', max_new_tokens=8192)\n",
    "\n",
    "# Async API\n",
    "# async_client = openllm.AsyncHTTPClient(\"http://127.0.0.1:8001\", timeout=120)\n",
    "# res = await async_client.generate(\"what is the weight of the earth?\", max_new_tokens=8192)\n",
    "print(res.outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streaming\n",
    "for it in client.generate_stream('What is the weight of the earth?', max_new_tokens=2048, n=2, best_of=2):\n",
    "  #print(f'index {it.index}: {it.text} (token: {it.token_ids})')\n",
    "  print(it.text, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open ai compatible endpoint\n",
    "\n",
    "import openai\n",
    "import os\n",
    "\n",
    "\n",
    "client = openai.OpenAI(base_url=os.getenv('OPENLLM_ENDPOINT', 'http://localhost:8001') + '/v1', api_key='na')\n",
    "models = client.models.list()\n",
    "print('Models:', models.model_dump_json(indent=2))\n",
    "model = models.data[0].id\n",
    "\n",
    "#os.environ[\"STREAM\"] = \"TRUE\"\n",
    "stream = str(os.getenv('STREAM', False)).upper() in ['TRUE', '1', 'YES', 'Y', 'ON']\n",
    "\n",
    "completions = client.completions.create(\n",
    "  prompt='Write me a tag line for an ice cream shop.', model=model, max_tokens=64, stream=stream\n",
    ")\n",
    "\n",
    "print(f'Completion result (stream={stream}):')\n",
    "if stream:\n",
    "  for chunk in completions:\n",
    "    text = chunk.choices[0].text\n",
    "    if text:\n",
    "      print(text, flush=True, end='')\n",
    "else:\n",
    "  print(completions.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenLLM\n",
    "\n",
    "\n",
    "llm = OpenLLM(server_url='http://localhost:8001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "template = 'What is a good name for a company that makes {product}?'\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=['product'])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "generated = llm_chain.run(product='mechanical keyboard')\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
